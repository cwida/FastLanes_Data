{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79516fca-944e-4f49-a9b4-660e435d2460",
   "metadata": {},
   "source": [
    "# Download the files that are <100MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ac05e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "from urllib.parse import urljoin\n",
    "import bz2\n",
    "\n",
    "url = 'https://homepages.cwi.nl/~boncz/NextiaJD/'\n",
    "\n",
    "def is_downloadable(url):\n",
    "    \"\"\"\n",
    "    Does the url contain a downloadable resource\n",
    "    \"\"\"\n",
    "    h = requests.head(url, allow_redirects=True)\n",
    "    header = h.headers\n",
    "    content_type = header.get('content-type', '')\n",
    "    if 'text' in content_type.lower() or 'html' in content_type.lower():\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def get_file_size(url):\n",
    "    \"\"\"\n",
    "    Get the size of the file at the given URL in MB\n",
    "    \"\"\"\n",
    "    response = requests.head(url, allow_redirects=True)\n",
    "    size = int(response.headers.get('content-length', 0))\n",
    "    return size / 1024 / 1024  # Convert to MB\n",
    "\n",
    "def download_and_decompress_file(url, folder_path, csv_filename):\n",
    "    \"\"\"\n",
    "    Download a bz2 compressed file from a given URL, decompress it, and save as CSV in its own folder\n",
    "    \"\"\"\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "\n",
    "    file_path = os.path.join(folder_path, csv_filename)\n",
    "    response = requests.get(url, stream=True)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        decompressed_data = bz2.decompress(response.content)\n",
    "        with open(file_path, 'wb') as f:\n",
    "            f.write(decompressed_data)\n",
    "        print(f\"Decompressed and saved {file_path}\")\n",
    "    else:\n",
    "        print(f\"Failed to download {url}\")\n",
    "\n",
    "# Get the webpage\n",
    "page = requests.get(url)\n",
    "soup = BeautifulSoup(page.text, 'html.parser')\n",
    "\n",
    "# Find all links on the page\n",
    "for link in soup.find_all('a'):\n",
    "    file_url = urljoin(url, link['href'])\n",
    "    \n",
    "    if is_downloadable(file_url):\n",
    "        size_mb = get_file_size(file_url)\n",
    "        \n",
    "        if size_mb < 100:  \n",
    "            original_filename = link['href']\n",
    "            if original_filename.endswith('.bz2'):\n",
    "                # Change the extension from .bz2 to .csv\n",
    "                csv_filename = original_filename[:-4] \n",
    "                folder_name = csv_filename.rsplit('.', 1)[0]\n",
    "                folder_path = os.path.join(os.getcwd(), folder_name)\n",
    "                \n",
    "                download_and_decompress_file(file_url, folder_path, csv_filename)\n",
    "\n",
    "print(\"Download and decompression completed for all files under 100MB.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810ea11f-0a2d-40a8-9c30-6dd6ff4018a9",
   "metadata": {},
   "source": [
    "# Download metadata file and delete unnecessary info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7f7f76-bfe0-48e8-9fdf-fbb221075e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(url)\n",
    "response.raise_for_status()  \n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "link = soup.find('a', href=lambda href: href and 'metadata.csv' in href)\n",
    "\n",
    "if link:\n",
    "    file_url = urljoin(url, link['href'])\n",
    "    file_response = requests.get(file_url)\n",
    "    file_response.raise_for_status() \n",
    "    with open('metadata.csv', 'wb') as file:\n",
    "        file.write(file_response.content)\n",
    "    print(\"The metadata.csv file has been downloaded successfully!\")\n",
    "else:\n",
    "    print(\"The metadata.csv file was not found on the page.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ce2d1d-79c0-44f2-a59e-a379b2357dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "\n",
    "df = pd.read_csv('metadata.csv')\n",
    "folders = [name for name in os.listdir('.') if os.path.isdir(name)]\n",
    "valid_filenames = [f\"{folder}.csv\" for folder in folders]\n",
    "df_filtered = df[df['filename'].apply(lambda x: any(x == valid_name for valid_name in valid_filenames))]\n",
    "df_filtered.drop(axis=1, labels='file_size', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a519c8-8243-421a-8494-144153bd7c45",
   "metadata": {},
   "source": [
    "# Keep only the first 64 * 1024 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ade2ec-fd71-4f3c-aedf-63bde0628083",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "def trim_csv(csv_file_path, max_rows):\n",
    "    \"\"\"\n",
    "    Reads a CSV file as a plain text file, respects multiline quotes as part of a single row,\n",
    "    retains only the first `max_rows` of data excluding the header, and rewrites the CSV file\n",
    "    with this subset. Prints a warning if the actual number of data rows in the file is less\n",
    "    than `max_rows`.\n",
    "\n",
    "    Parameters:\n",
    "    - csv_file_path (str): Path to the CSV file to process.\n",
    "    - max_rows (int): Maximum number of data rows to keep in the CSV file, excluding the header.\n",
    "\n",
    "    Returns:\n",
    "    - None: Modifies the file specified by `csv_file_path` directly and does not return a value.\n",
    "\n",
    "    Side effects:\n",
    "    - Rewrites the CSV file at `csv_file_path` with up to `max_rows` of data, preserving the header.\n",
    "    - Prints a warning if the file contains fewer data rows than `max_rows`.\n",
    "    \"\"\"\n",
    "    temp_file_path = csv_file_path + \".tmp\"\n",
    "    try:\n",
    "        with open(csv_file_path, 'r') as read_file, open(temp_file_path, 'w') as write_file:\n",
    "            header = read_file.readline()\n",
    "            write_file.write(header)  # Write the header to the temp file\n",
    "            rows_written = 0\n",
    "            in_quote = False\n",
    "\n",
    "            for line in read_file:\n",
    "                if rows_written >= max_rows:\n",
    "                    break  \n",
    "                write_file.write(line)\n",
    "                # Check if the line contains an odd number of quotes, flipping the in_quote flag accordingly\n",
    "                if line.count('\"') % 2 != 0:\n",
    "                    in_quote = not in_quote\n",
    "                if not in_quote and line.endswith('\\n'):\n",
    "                    rows_written += 1\n",
    "\n",
    "        if rows_written < max_rows:\n",
    "            print(f\"Warning: The file {csv_file_path} has only {rows_written} data rows, which is less than {max_rows}.\")\n",
    "\n",
    "        os.replace(temp_file_path, csv_file_path)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing {csv_file_path}: {e}\")\n",
    "        if os.path.exists(temp_file_path):\n",
    "            os.remove(temp_file_path)\n",
    "\n",
    "\n",
    "# Increase the maximum field size limit\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "cwd = os.getcwd()\n",
    "folders = [f for f in os.listdir(cwd) if os.path.isdir(os.path.join(cwd, f))]\n",
    "\n",
    "for folder in folders:\n",
    "    if folder == '.ipynb_checkpoints':\n",
    "        continue\n",
    "    # Construct the file path for the CSV file in each folder\n",
    "    csv_file_path = os.path.join(cwd, folder, f'{folder}.csv')\n",
    "    print(f\"Processing {folder}\")\n",
    "    \n",
    "    if os.path.isfile(csv_file_path):\n",
    "        trim_csv(csv_file_path, 64 * 1024)\n",
    "    else:\n",
    "        print(f\"No CSV file found for folder: {folder}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67522ee2-ac1f-4836-8ef2-d6ec6b970934",
   "metadata": {},
   "source": [
    "# Fix the faulty entries in Chicago_Crimes_2001_to_2004.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34164620-7fa3-497f-bc13-e1b1f11cdc3f",
   "metadata": {},
   "source": [
    "The faulty columns in row 1602849 are 'Y Coordinate', 'Year', 'Updated On'. But only 'Y Coordinate' column in the file contains None values. Thus, in row 1602849 we replace faulty 'Y Coordinate' by empty block, while 'Year' and 'Updated On' entries are taken from row 1602848"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6fbdeb-8c63-4635-b766-f5d35b8eb5c9",
   "metadata": {},
   "source": [
    "This is only necessary when working with more than 64 * 1024 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf95fe5-be54-4418-849b-4921b39bfe75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#file_path = os.getcwd() + '/Chicago_Crimes_2001_to_2004/Chicago_Crimes_2001_to_2004.csv'\n",
    "#old_line = '3629582,1423259,G139165,03/10/2001 11:30:00 PM,035XX S FEDERAL ST,1340,CRIMINAL DAMAGE,TO STATE SUP PROP,CHA PARKING LOT/GROUNDS,True,False,211,2.0,,,14,1176246.0,18 08:55:02 AM,41.789832136,-87.672973835,\"(41.789832136, -87.672973835)\"\\n'\n",
    "#new_line = '3629582,1423259,G139165,03/10/2001 11:30:00 PM,035XX S FEDERAL ST,1340,CRIMINAL DAMAGE,TO STATE SUP PROP,CHA PARKING LOT/GROUNDS,True,False,211,2.0,,,14,1176246.0,,2001,08/17/2015 03:03:40 PM,41.789832136,-87.672973835,\"(41.789832136, -87.672973835)\"\\n'\n",
    "#\n",
    "## Read the file and replace the line\n",
    "#with open(file_path, 'r') as file:\n",
    "#    lines = file.readlines()\n",
    "#\n",
    "## Replace the line\n",
    "#lines = [new_line if line == old_line else line for line in lines]\n",
    "#\n",
    "## Write the changes back to the file\n",
    "#with open(file_path, 'w') as file:\n",
    "#    file.writelines(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfc72c8-e70d-4b1b-bcac-bfe230b83ad2",
   "metadata": {},
   "source": [
    "# Fix the faulty entries in Chicago_Crimes_2005_to_2007.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40f6260-9327-401a-86d9-9e14525ca704",
   "metadata": {},
   "source": [
    "Here, in row 533719 we have a problem with 'Location' entry. But since location can be inferred by the 'Longitude' and 'Latitude' entries, we can fix this manually"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1922833b-a562-4917-9d36-a4f162f29912",
   "metadata": {},
   "source": [
    "This is only necessary when working with more than 64 * 1024 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad1991c-f7f2-414a-9e54-01edda5a85e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#file_path = os.getcwd() + '/Chicago_Crimes_2005_to_2007/Chicago_Crimes_2005_to_2007.csv'\n",
    "#old_line = '537288,5601758,HN409865,06/16/2007 08:15:00 PM,020XX E 94TH ST,1330,CRIMINAL TRESPASS,TO LAND,OTHER RAILROAD PROP / TRAIN DEPOT,False,False,413,4.0,8.0,48.0,26,1191237.0,1843038.0,2007,04/15/2016 08:55:02 AM,41.724300463,-87.575094193,\"(41.724300463, -87.5,ID,Case Number,Date,Block,IUCR,Primary Type,Description,Location Description,Arrest,Domestic,Beat,District,Ward,Community Area,FBI Code,X Coordinate,Y Coordinate,Year,Updated On,Latitude,Longitude,Location\\n'\n",
    "#new_line = '537288,5601758,HN409865,06/16/2007 08:15:00 PM,020XX E 94TH ST,1330,CRIMINAL TRESPASS,TO LAND,OTHER RAILROAD PROP / TRAIN DEPOT,False,False,413,4.0,8.0,48.0,26,1191237.0,1843038.0,2007,04/15/2016 08:55:02 AM,41.724300463,-87.575094193,\"(41.724300463, -87.575094193)\"\\n'\n",
    "#\n",
    "## Read the file and replace the line\n",
    "#with open(file_path, 'r') as file:\n",
    "#    lines = file.readlines()\n",
    "#\n",
    "## Replace the line\n",
    "#lines = [new_line if line == old_line else line for line in lines]\n",
    "#\n",
    "## Write the changes back to the file\n",
    "#with open(file_path, 'w') as file:\n",
    "#    file.writelines(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebbd983-d52b-4f90-9604-dfc537379631",
   "metadata": {},
   "source": [
    "# Clean az.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9adb8a-6c22-41c6-a3dc-26061bc77ad8",
   "metadata": {},
   "source": [
    "The “POSTCODE” column sometimes contains “ “, which by our assumption denotes NaN. Thus, we replace “ “ by void so that the column type is BIGINT instead of VARCHAR.\n",
    "\n",
    "Moreover, row 2423531 contains “GLAMU” in “POSTCODE” row, which is not a valid postcode. Thus, we set that value to be void too. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a109250e-e584-4f6b-b793-1ea353e4ba62",
   "metadata": {},
   "source": [
    "This is only necessary when working with more than 64 * 1024 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a3613b-3ad8-4625-b028-6a5223f83ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#file_path = os.getcwd() + '/az/az.csv'\n",
    "#\n",
    "## Read the file and store lines in memory\n",
    "#with open(file_path, 'r', encoding='utf-8') as file:\n",
    "#    lines = file.readlines()\n",
    "#\n",
    "## Perform the replacements\n",
    "#modified_lines = [line.replace(\", ,\", \",,\") for line in lines]\n",
    "#modified_lines = [line.replace(\",GLAMU,\", \",,\") for line in modified_lines]\n",
    "#modified_lines = [line.replace(\",GARY,,a017b5cb9489f7de\", \",,,a017b5cb9489f7de\") for line in modified_lines]\n",
    "#modified_lines = [line.replace(\",GARY,,4510b55559cff995\", \",,,4510b55559cff995\") for line in modified_lines]\n",
    "#\n",
    "#\n",
    "## Overwrite the original file with modified lines\n",
    "#with open(file_path, 'w', encoding='utf-8') as file:\n",
    "#    file.writelines(modified_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77677d84-358e-4b5b-9342-eac9b638caa2",
   "metadata": {},
   "source": [
    "# Change the delimiters in csv files to |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2f6810-47d6-410f-a916-d81c50503e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df_filtered.iterrows():\n",
    "    folder_name = row['filename'][:-4]\n",
    "    original_delimiter = row['delimiter']\n",
    "    file_path = os.path.join(os.getcwd(), folder_name, folder_name + '.csv')  # Construct file path\n",
    "\n",
    "    if os.path.exists(file_path):\n",
    "        # Read in the old file\n",
    "        with open(file_path, mode='r', encoding='utf-8') as file:\n",
    "            reader = csv.reader(file, delimiter=original_delimiter if original_delimiter != '\\\\t' else '\\t')\n",
    "            rows = list(reader)\n",
    "\n",
    "        # Write out the new file\n",
    "        with open(file_path, mode='w', encoding='utf-8') as file:\n",
    "            writer = csv.writer(file, delimiter='|')\n",
    "            writer.writerows(rows)\n",
    "        \n",
    "        print(file_path, 'processed')\n",
    "    else:\n",
    "        print(\"Error with\", file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0d676b-ed98-4932-9cce-05b2432f797e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overwrite the original metadata file file with the filtered data\n",
    "df_filtered.drop('delimiter', axis=1, inplace=True)\n",
    "df_filtered.to_csv('metadata.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bcd8e63-d566-441b-88b7-a3ecdbe6ddf0",
   "metadata": {},
   "source": [
    "# Generate a schema for files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7fa5bfe-a681-4681-88f2-5ad5225d9067",
   "metadata": {},
   "source": [
    "When we generate the schema.yaml, we also get rid of the header of the .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b43312-53fe-4076-bf42-1069f8484464",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import yaml\n",
    "\n",
    "max_rows = 1024 * 64\n",
    "cwd = os.getcwd()\n",
    "folders = [f for f in os.listdir(cwd) if os.path.isdir(os.path.join(cwd, f))]\n",
    "\n",
    "# This is for correct .yaml formatting \n",
    "class MyDumper(yaml.Dumper):\n",
    "    def increase_indent(self, flow=False, indentless=False):\n",
    "        return super(MyDumper, self).increase_indent(flow, False)\n",
    "\n",
    "for folder in folders:\n",
    "    csv_file_path = os.path.join(cwd, folder, f'{folder}.csv')\n",
    "    \n",
    "    if os.path.isfile(csv_file_path):\n",
    "        con = duckdb.connect(database=':memory:') \n",
    "\n",
    "        # DuckDB cannot correctly guess the formatting of some files. Thus, we sometimes help it \n",
    "        # The commented out lines may be useful when row number of files is > 64 * 1024\n",
    "        if folder == 'wowah_data':\n",
    "            con.execute(f\"CREATE TABLE my_table AS SELECT * FROM read_csv('{csv_file_path}', timestampformat='%m/%d/%y %H:%M:%S')\")\n",
    "        elif folder == 'us_perm_visas':\n",
    "            con.execute(f\"CREATE TABLE my_table AS SELECT * FROM read_csv('{csv_file_path}',\" + \"\"\" types = {\n",
    "            'employer_postal_code': 'VARCHAR'})\"\"\") \n",
    "            #'wage_offer_from_9089': 'VARCHAR', \n",
    "            #'wage_offer_to_9089': 'VARCHAR', \n",
    "            #'pw_amount_9089': 'VARCHAR'})\"\"\")\n",
    "        elif folder == 'business-licences':\n",
    "            con.execute(f\"CREATE TABLE my_table AS SELECT * FROM read_csv('{csv_file_path}',\" + \"\"\" types = {\n",
    "            'House': 'VARCHAR'})\"\"\")\n",
    "        else:\n",
    "            con.execute(f\"CREATE TABLE my_table AS SELECT * FROM read_csv('{csv_file_path}')\")\n",
    "\n",
    "        # Check if the number of rows matches max_rows\n",
    "        row_count_result = con.execute(\"SELECT COUNT(*) FROM my_table\").fetchone()[0]\n",
    "        if row_count_result != max_rows:\n",
    "            raise ValueError(f\"Error: The table created from {csv_file_path} contains {row_count_result} rows, which does not match the expected {max_rows} rows.\")\n",
    "        \n",
    "        # Retrieve and convert the schema to YAML\n",
    "        schema_result = con.execute(\"DESCRIBE my_table\").fetchall()\n",
    "        # Transform the schema result into the desired format\n",
    "        schema_formatted = {'columns': [{'name': col[0], 'type': col[1]} for col in schema_result]}\n",
    "        schema_yaml = yaml.dump(schema_formatted, sort_keys=False)\n",
    "        \n",
    "        # Save the YAML\n",
    "        yaml_file_path = os.path.join(cwd, folder, 'schema.yaml')\n",
    "        with open(yaml_file_path, 'w') as f:\n",
    "            yaml.dump(schema_formatted, f, sort_keys=False, Dumper=MyDumper, default_flow_style=False)\n",
    "        \n",
    "        # After saving the schema, remove the header from the CSV file\n",
    "        # Read the CSV file again, this time as a list of lines\n",
    "        with open(csv_file_path, 'r') as file:\n",
    "            lines = file.readlines()\n",
    "        \n",
    "        # Write the lines back, excluding the first line (header)\n",
    "        with open(csv_file_path, 'w') as file:\n",
    "            file.writelines(lines[1:])\n",
    "        \n",
    "        print(f\"Made schema.yaml and removed header for: {csv_file_path}\")\n",
    "        \n",
    "        con.close()\n",
    "    else:\n",
    "        print(f\"No CSV file found for folder: {folder}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6099c892-e7ec-4b09-bb31-7c24829edc61",
   "metadata": {},
   "source": [
    "# Rename the files to work with C++"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc59baf-d411-44df-8c41-ed3d7d322d15",
   "metadata": {},
   "source": [
    "C++ doesn't support file names with '-' and '.' symbols, and at some point we do want to have files for each file name. Thus, we replace these symbols in files/folders now with underscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbc1e030-ae24-43b2-ba0d-f3fe01bc3739",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "def rename_folders_and_files(directory):\n",
    "    \"\"\"\n",
    "    Renames folders and files within the specified directory.\n",
    "    Only affects folders whose names contain '-' or '.', replacing these characters with '_'.\n",
    "    Moves all files from the old folder to the newly named folder if renaming is necessary.\n",
    "    \"\"\"\n",
    "    directory = os.path.abspath(directory)\n",
    "    folders = [d for d in os.listdir(directory) if os.path.isdir(os.path.join(directory, d))]\n",
    "\n",
    "    for folder in folders:\n",
    "        if '-' in folder or '.' in folder:\n",
    "            new_folder_name = folder.replace('-', '_').replace('.', '_')\n",
    "            old_folder_path = os.path.join(directory, folder)\n",
    "            new_folder_path = os.path.join(directory, new_folder_name)\n",
    "\n",
    "            os.makedirs(new_folder_path, exist_ok=True)\n",
    "\n",
    "            old_csv_path = os.path.join(old_folder_path, f\"{folder}.csv\")\n",
    "            new_csv_path = os.path.join(new_folder_path, f\"{new_folder_name}.csv\")\n",
    "            if os.path.exists(old_csv_path):\n",
    "                shutil.move(old_csv_path, new_csv_path)\n",
    "\n",
    "            for item in os.listdir(old_folder_path):\n",
    "                old_item_path = os.path.join(old_folder_path, item)\n",
    "\n",
    "                if old_item_path != old_csv_path:\n",
    "                    new_item_path = os.path.join(new_folder_path, item)\n",
    "                    shutil.move(old_item_path, new_item_path)\n",
    "\n",
    "            if not os.listdir(old_folder_path):\n",
    "                os.rmdir(old_folder_path)\n",
    "            else:\n",
    "                print(f\"Folder not empty: {old_folder_path}\")\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "\n",
    "current_directory = '.'\n",
    "rename_folders_and_files(current_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c349fa72-7c3a-45d6-8f5e-43cb6a987cfd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
